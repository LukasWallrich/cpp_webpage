## 3. Methods for data collection

:::video
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/9uMckCQxcQ0?rel=0&modestbranding=1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<a href="presentations/03-methods.pptx" class="download-btn">
  <svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M21 15v4a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2v-4"/><polyline points="7 10 12 15 17 10"/><line x1="12" y1="15" x2="12" y2="3"/></svg>
  Download slides (PPTX)
</a>

Part 3: Methods and ethics
- What counts as data? Facts, feelings, and process
- Common data collection methods
- Ethics in consultancy data collection
- Going beyond the surface (iceberg models)
- Feeding back findings to clients
:::

### What is data?

:::summary
Data encompasses:
- **Facts** — what everyone accepts as objective
- **Feelings** — subjective perceptions (facts to those experiencing them)
- **Process** — how things are done (style, politics, decision-making)

Data can be primary/secondary and quantitative/qualitative—often complementary.
:::

- **The 'Facts'**: what everyone accepts as objective facts
- **The 'Feelings'**: how people feel about what's happening (subjective perceptions, but facts to them)
- **The 'Process'**: how things are done, rather than what is done (e.g., how a problem is being managed, style of management, politics, who does what to whom, how decisions are taken)

As in research methods, we might categorise data as:
- Primary vs secondary
- Quantitative vs qualitative

These are often used as complementary approaches [@omahoney2013]. Many different methods exist, each with its pros and cons, and demands in terms of skills and effort required.

### Common data collection methods

:::summary
Key methods: interviews, surveys, focus groups, document analysis, observation, literature reviews.

Each has trade-offs (see table below). A key task: **triangulate** findings across multiple sources.
:::

| Method | Best for | Key considerations |
|--------|----------|-------------------|
| **Interviews** | Deep understanding, exploring unknowns | Time-intensive; need to triangulate |
| **Questionnaires/surveys** | Large numbers, standardised data | Response rates; question design |
| **Focus groups/workshops** | Exploring group dynamics, testing ideas | Facilitation skills; group effects |
| **Document analysis** | Understanding history, official positions | May not reflect reality |
| **Observation/shadowing** | Seeing actual behaviour | Observer effects; time-consuming |
| **Literature reviews** | Context, benchmarks, best practice | Quality varies; may be outdated |
| **Experiments** | Establishing causation | Often impractical; ethical issues |

A key task with interviews and other qualitative methods is to **triangulate**: check findings across multiple sources, including internal and external perspectives.

### Consultancy and research ethics

:::summary
Even if consultancy is less formal than research, ethical principles apply:
- Informed consent, voluntary participation, privacy
- Right to withdraw, safety, confidentiality
- **Data protection** — UK GDPR applies; consultancy can't rely on "public interest"
:::

As discussed in Part 1, consultancy projects tend toward the "action approach" rather than research rigour—favouring quicker methods, less specific questions, and politically-influenced sampling. This creates ethical considerations:

- **Informed consent**: participants should know what they're participating in
- **Voluntary participation**: people shouldn't be coerced
- **Right to privacy**: respect personal boundaries
- **Right to withdraw**: from part or all of the process
- **Right to safety**: avoid embarrassment, stress, discomfort
- **Confidentiality/anonymity**: for individuals and organisations
- **Data protection**: UK GDPR applies; consultancy cannot rely on the "public interest" basis that protects academic research, making consent requirements more stringent

The question to ask: *Which of these might not apply in a consultancy setting?* The honest answer is: all of them should apply, but the pressures of commercial work often erode them.

These principles can create genuine dilemmas: What do you do when a client asks you to share individual interview responses? When they want to use diagnostic data to identify "problem" employees? When participation in your process is technically voluntary but practically compelled? We'll explore these ethical tensions in more depth later in the course, but for now, be aware that data collection creates power dynamics that consultants must navigate thoughtfully.

### The value of experiments

Anderson and Simester [-@andersonsimester2011] argue that businesses should use experiments more often, and the same logic applies to consultancy:

:::framework
**Why experiments matter for consultants**

Consulting aims to cause change, and causal questions are best answered by experiments. While analytics helps understand the past, experiments test what actually works going forward. Anderson and Simester found that "feedback from even a handful of experiments can yield immediate and dramatic improvements."

Most organisations resist experimentation because it feels slower than acting on intuition. But intuition-based decisions are often wrong; experiments reveal this before expensive rollouts.
:::

Guidelines for consulting experiments:
- Measure real reactions from real individuals
- Keep it simple; most experiments will fail, while others can be scaled up
- Create a proof of concept
- Consider "natural experiments" (requires luck and creativity)

<!-- Summary generated from PDF: sources/Anderson-2011.pdf -->
:::source[andersonsimester2011]
Eric T. Anderson and Duncan Simester argue that most companies can generate greater value from simple, controlled business experiments than from complex analytics programmes, because experiments enable managers to draw clear causal conclusions about customer responses to business changes.

**Key findings/arguments**:
- Most firms lack the technical skills to implement full-scale analytics programmes effectively, whilst experimentation is a skill nearly any manager can acquire
- The "test and learn" approach (comparing outcomes between treatment and control groups) produces easily interpreted results with clear causality
- Successful experimentation requires two essential components: randomised control groups and a feedback mechanism to observe customer responses
- Seven rules improve experimental design: focus on individuals with short-term measurable outcomes, keep implementations simple, establish proof-of-concept before refinement, segment data to identify subgroup effects, encourage creative "what-if" thinking, measure all relevant effects across channels, and identify natural experiments created by external business changes
- Organisational culture poses the largest obstacle to experimentation; shifting from intuition-based to experimentation-based decision making requires senior leadership commitment and willingness to accept that most experiments (perhaps 95%) will not yield profitable outcomes
- Companies should run many small-scale experiments rather than few large ones, focusing resources on scaling the "golden tickets" (the few experiments that demonstrate genuinely profitable innovations)

**Key insight**: Organisations that embrace experimentation as a core management discipline, supported by adequate infrastructure and governance, can systematically discover breakthrough improvements to pricing, promotions, and product strategies that intuitive decision-making alone would never reveal.
:::

### Going beyond the surface

:::summary
Organisations and individuals have visible and hidden layers:
- **Corporate iceberg**: structures & stated values above unstated assumptions
- **Individual iceberg**: job titles above fears, hopes, values, biases
:::

:::definition
"Time spent on reconnaissance is seldom wasted."
Army maxim, applicable to consultancy
:::

![Iceberg Models](content/img/iceberg-model.png)

**Corporate culture iceberg**: What's visible (structures, processes, stated values) sits above a larger mass of unstated assumptions, beliefs, and norms.

**Individual positions iceberg**: Job titles and stated opinions sit above fears, hopes, values, experiences, biases, and needs.

For structured techniques to dig beneath the surface (including root cause analysis and Six Sigma), see the [diagnostic techniques section](#part5).

### Feeding back findings

:::summary
Block [-@block2011]: feedback meetings are where diagnosis meets action.

Key principles: co-own the data, start with strengths, be specific, invite challenge.
:::

How you present diagnostic findings shapes whether clients can hear them, accept them, and act on them. Block [-@block2011] devotes significant attention to feedback meetings (Chapter 11), emphasising that this is where diagnosis meets action.

**Key principles for effective feedback:**

- **Co-own the data**: Present findings as shared discoveries, not verdicts delivered by an outside expert. The client needs to feel ownership of the diagnosis to act on it.
- **Start with strengths**: What's working well? Positive deviance and appreciative inquiry approaches (see [dealing with resistance](#part4)) remind us that organisations have resources to build on.
- **Be specific and grounded**: Abstract conclusions ("communication is poor") are less actionable than concrete observations ("the weekly team meeting focuses on updates, with no time for problem-solving").
- **Invite challenge**: Create space for the client to disagree, add context, or reframe. Their pushback is data too.

**Participatory feedback techniques:**

Several methods help make feedback sessions genuinely collaborative:

- **Gallery walks**: Post key findings on flip charts around a room. Participants circulate, adding comments, questions, and reactions. This distributes voice and surfaces diverse responses before group discussion.
- **Co-design sessions**: Rather than presenting recommendations, present diagnostic themes and facilitate joint problem-solving. The client brings contextual knowledge; you bring analytical frameworks.
- **Prioritisation exercises**: When there are multiple findings, involve the client in deciding what matters most. Dot voting, impact/effort matrices, or structured ranking help build consensus.

We'll explore feedback and recommendation presentation in more depth when we cover **designing solutions** in Week 4.

:::reflect
Think about a recent problem at work. What was visible on the surface? What might have been hidden beneath?
:::

### Additional reading: AI-supported approaches to data collection

**Note**: This field is evolving rapidly. The guidance below reflects the state of practice in early 2026, but capabilities and best practices continue to change. Treat this as an invitation to experimentation rather than established proven approaches.

AI tools are transforming three key areas of diagnostic data gathering: text analysis, interviewing, and document mining. Recent evidence suggests these tools can substantially accelerate early-stage diagnostic work, though human expertise remains essential for interpretation and strategic insight.

#### LLM-powered text analysis

Large language models can now analyse unstructured text—interview transcripts, survey comments, emails, internal communications—far faster than manual coding. Teams report over 90% time savings: analysis that took 30 hours manually can now be completed in minutes.

Research confirms quality can match human analysis. Liu and Sun [-@liusun2025] used GPT-4 to categorise stakeholder interview data and found 77–96% agreement with human coders, outperforming older NLP methods by more than 25%. The AI-generated coding was also more aligned with expert sentiment analysis than traditional lexicon-based techniques.

**Accessible tools**: You don't need enterprise platforms to experiment. Open-source workflows like GATOS [@katz2026] use freely available generative models to assist thematic analysis, automatically generating codebooks comparable to human-developed ones. Traditional qualitative software (QDA Miner, NVivo, Dovetail) is also adding LLM features. Even feeding documents into ChatGPT or Claude for initial theme identification can accelerate early exploration.

:::source[katz2026]
Katz, A., Fleming, G.C. & Main, J.B. (2026) "Thematic analysis with open-source generative AI and machine learning: a new method for inductive qualitative codebook development", *Humanities and Social Sciences Communications*. DOI: [10.1057/s41599-026-06508-5](https://doi.org/10.1057/s41599-026-06508-5)

This paper presents the GATOS (Generative AI-enabled Theme Organization and Structuring) workflow, which uses open-source LLMs and NLP tools to facilitate aspects of thematic analysis. **Key findings**: Testing on three synthetic datasets showed the workflow could generate themes that closely matched original sub-themes—fewer than 5% lacked a clear match, under 2% had no close match. The workflow summarises text, embeds and clusters similar points, generates codes while checking against the existing codebook to avoid redundancy, then consolidates codes into themes.

**Key insight**: Open-source tools can approximate key steps in thematic analysis, making AI-assisted qualitative coding accessible without enterprise platforms or data privacy concerns from third-party APIs. The approach works best for large-scale coding where manual analysis would be impractical.
:::

:::source[liusun2025]
Liu, A. & Sun, M. (2025) "From Voices to Validity: Leveraging Large Language Models (LLMs) for Textual Analysis of Policy Stakeholder Interviews", *AERA Open*, 11. https://arxiv.org/abs/2312.01202

This study demonstrates that GPT-4 can effectively complement human expertise in analysing stakeholder interviews about K-12 education policy. **Key findings**: LLM coding matched human coding at 77.89% for specific themes, improving to 96.02% for broader thematic categories—surpassing traditional NLP methods by over 25%. The researchers used a mixed-methods approach combining human domain knowledge with unsupervised topic modelling to develop coding frameworks and refine prompts iteratively.

**Key insight**: Human-computer collaboration enhances research efficiency while maintaining validity and interpretability. The approach works best when humans provide domain expertise for framework development while AI handles the scale of coding.
:::

#### AI-driven interviews and surveys

Conversational AI can now conduct structured interviews without human moderators. Kaiyrbekov et al. [-@kaiyrbekov2025] demonstrated LLM-powered phone agents that call participants, dynamically ask questions, and record answers—with extracted survey responses achieving 98% accuracy despite imperfect transcription.

These systems adapt questions based on responses, much like skilled human interviewers. Platforms report that breakthrough insights often come from dynamic follow-up questions the AI asks when detecting something interesting or ambiguous.

A notable finding: **people may be more candid with AI interviewers**. Studies suggest respondents feel more comfortable giving honest answers to an AI than to a human moderator. Without fear of judgement, participants volunteer sensitive feedback more readily, potentially revealing issues traditional interviews miss.

**Practical applications**: AI could handle initial structured interviews across hundreds of employees simultaneously, identifying broad themes, with consultants following up in person on complex or sensitive topics.

:::source[kaiyrbekov2025]
Kaiyrbekov, K., Dobbins, N.J. & Mooney, S.D. (2025) "Automated survey collection with LLM-based conversational agents", *JAMIA Open*, 8(5), ooaf103. DOI: [10.1093/jamiaopen/ooaf103](https://doi.org/10.1093/jamiaopen/ooaf103) | [Open access PDF](https://academic.oup.com/jamiaopen/article-pdf/8/5/ooaf103/65043536/ooaf103.pdf)

This research demonstrates that AI-powered phone surveys can effectively collect data. The team achieved 98% accuracy in extracting survey responses using GPT-4o, despite a 7.7% word error rate in speech transcription. **Key findings**: The system maintained high accuracy even with imperfect transcription, suggesting LLMs can infer correct responses from context. The pilot cost only $0.75 per survey, demonstrating scalability. Performance was comparable for native and non-native English speakers.

**Key insight**: Conversational AI agents can scale data collection while maintaining accuracy. The robustness to transcription errors makes this practical for real-world deployment.
:::

#### Mining internal documents (RAG approach)

Retrieval-Augmented Generation (RAG)—often called "chat with your data"—lets AI search and synthesise information from internal knowledge bases: policies, manuals, reports, feedback logs. Instead of manually reading dozens of documents, consultants can ask questions in natural language and get synthesised answers with sources cited.

Industry reports suggest RAG reduces time spent searching for information by approximately 40%. A consultant diagnosing a process issue could ask: "What are the documented steps and common bottlenecks in our order fulfilment process?" and receive an answer drawn from technical SOPs, logs, and past communications—work that previously required weeks of document requests.

Major consultancies are building internal RAG tools. McKinsey developed "Lilli" for consultants to query the firm's vast knowledge base. Open-source frameworks (LangChain, LlamaIndex) let you build similar systems for case study materials or client documents.

**Key caveat**: Data quality matters—if the knowledge base contains outdated information, so will the AI's answers. Best practice includes having the AI cite sources so consultants can verify key facts.

#### The "floor and ceiling" principle

A useful mental model from practitioners: **AI raises the floor but not the ceiling of insight quality**. These tools can rapidly get you perhaps 70% of the insight, saving substantial time on basic data gathering. But the final 100%—strategic implications, nuanced interpretation, relationship-building—still requires human expertise.

The best outcomes come from hybrid approaches: AI handling scale and pattern-recognition while humans handle validation, sense-making, and the relational work of consulting.

#### Practical guidance

1. **Use AI as complement, not replacement**: Let AI handle initial coding or document review; apply human judgement to interpretation

2. **Verify outputs**: Treat AI-generated themes as hypotheses to check against source data, not conclusions

3. **Be transparent with clients**: If using AI in diagnosis, clients should know—some have concerns about data security

4. **Recognise limitations**: Sensing political dynamics, noticing what people *don't* say, picking up emotional undercurrents—these require human presence

5. **Experiment actively**: Many tools are freely accessible (open-source models, ChatGPT, Claude). Try simulating interviews by prompting AI to role-play as employees or customers—it's a low-cost way to test questions before fieldwork

6. **Stay current**: Practices cutting-edge today may be standard or obsolete within months

:::reflect
If you were using AI to help analyse 50 interview transcripts, how would you design the process to get the benefits of speed while maintaining analytical rigour? What checks would you build in?
:::
