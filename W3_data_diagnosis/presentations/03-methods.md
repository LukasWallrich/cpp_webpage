# Part 3: Methods and Ethics
**Target duration: ~18 minutes**

## Narrative Arc
This part moves from the *what* (types of data) through *how* (methods) to *why it matters* (ethics). We then pivot to the crucial insight that surface-level data collection isn't enough—we need to go deeper. This sets up Part 4's focus on techniques for doing so.

---

### Slide 16: Data Types and Choices
**Content**: Brief taxonomy

- Primary vs Secondary
- Quantitative vs Qualitative
- Often complementary—"both/and" rather than "either/or"

**Key talking points**:
- Different methods suit different questions (facts, feelings, and process)

- Choice depends on: what you need to learn, resources available, client culture
- No single "best" method

---

### Slide 17: Common Data Collection Methods
**Content**: Visual overview of options

**Figure**: Methods arranged by trade-offs (depth vs breadth, access vs scale)

| Method | Best for | Key consideration |
|--------|----------|-------------------|
| Interviews | Deep understanding, exploring unknowns | Time-intensive; triangulate |
| Questionnaires | Large numbers, standardised data | Response rates; question design |
| Focus groups | Group dynamics, testing ideas | Facilitation; group effects |
| Document analysis | History, official positions | May not reflect reality |
| Observation/shadowing | Actual behaviour | Observer effects |
| Literature reviews | Context, benchmarks | Quality varies |
| Experiments | Establishing causation | Often impractical |

**Key talking points**:
- Triangulation is essential—cross-check across sources
- Mix internal and external perspectives
- Written materials detail these; for now, focus on principles

---

### Slide 17b: AI Tools for Data Gathering (Emerging)
**Content**: Brief overview—this is an invitation to experiment, not established practice

**Note**: This field is evolving rapidly. What follows reflects early 2026; capabilities continue to change.

**Most promising current applications**:
- **LLM text analysis**: Interview transcripts, survey comments, internal communications—recent research shows 77–96% agreement with human coders; significant time savings
- **AI-driven interviews**: Conversational agents that adapt questions dynamically; some evidence respondents may be more candid with AI than human interviewers
- **Document mining (RAG)**: "Chat with your data" across policies, reports, knowledge bases

**Key principle**: "AI raises the floor, not the ceiling"
- Good for rapidly getting initial insight at scale
- Human expertise still essential for strategic interpretation, relationship-building, and sensing what's unsaid

**Key talking points**:
- This is worth experimenting with, but we don't yet have established best practices
- Many tools are freely accessible (open-source models, ChatGPT, Claude)
- Always verify AI outputs against source data
- The relational work of consulting—building trust, sensing politics—remains irreplaceable
- Written materials cover this in more depth for those interested

---

### Slide 18: The Value of Experiments (Anderson & Simester, 2011)
**Content**: Make the case for testing, not just analysing

**Core argument**:
- Consultants want to cause change
- Causal questions are best answered by experiments
- Analytics shows what happened; experiments show what works

**Guidelines**:
- Measure real reactions from real individuals
- Keep it simple—most experiments fail, scale up the winners
- Create proof of concept before major rollout
- Consider "natural experiments" (requires luck and creativity)

**Key talking points**:
- Experiments are underused in consulting
- Even small pilots can test assumptions before expensive rollouts
- Counter-intuitive: slowing down to test can be faster overall

---

### Slide 19: Consultancy and Research Ethics
**Content**: Ethics apply even without IRB oversight

**Research ethics foundations (all apply to consulting):**
- Right to informed consent
- Voluntary participation
- Right to privacy
- Right to withdraw
- Right to safety (avoid embarrassment, stress)
- Confidentiality/anonymity
- Right not to be deceived
- Avoidance of coercion

**Question to audience**: Which might not apply in consultancy?
**Answer**: All SHOULD apply—but commercial pressures often erode them

**Additional consideration**: UK GDPR applies more strictly to consultancy than research (no "public interest" exemption)

**Key talking points**:
- Ask yourself: would you be comfortable if participants knew exactly how their data would be used?
- We'll explore ethical tensions in more depth later in the module
- Essential for your MSc Research—and for professional practice

---

### Slide 20: Feeding Back Findings
**Content**: Data collection without effective feedback is incomplete

**Key principles (Block, 2010):**
- **Co-own the data**: Present as shared discoveries, not expert verdicts
- **Start with strengths**: What's working? (sets up Part 4 on positive approaches)
- **Be specific and grounded**: "Communication is poor" → "Weekly meetings have no time for problem-solving"
- **Invite challenge**: Create space for disagreement—their pushback is data too

**Key talking points**:
- HOW you present findings shapes whether clients can hear them
- The same diagnostic insight can mobilise action or trigger defensiveness
- We'll cover feedback in more depth when we reach Week 4 (designing solutions)

---

### Slide 21: Participatory Feedback Techniques
**Content**: Brief overview of collaborative methods

- **Gallery walks**: Post findings on flip charts; participants circulate and react before discussion
- **Co-design sessions**: Present themes, facilitate joint problem-solving
- **Prioritisation exercises**: Involve client in deciding what matters most (dot voting, impact/effort matrices)

**Key talking points**:
- These techniques distribute voice and build ownership
- Reduce the "expert delivers verdict" dynamic
- Part 4 will explore how positive approaches to diagnosis can reduce defensive reactions

---

### Slide 22: Part 3 Summary
**Content**: Recap and preview

**What we covered:**
- Common data collection methods and when to use them
- Ethics apply even without formal oversight
- How you feed back findings matters as much as what you find

**Reflection**: Think of a time you were asked to change something about how you work. What made the difference between a call to change you could hear versus one that triggered defensiveness?

**Written materials**: The written content covers data collection methods in depth, including interview techniques and observation approaches.

**Preview of Part 4:**
- Why surface data isn't enough—the iceberg model
- Structured techniques for going deeper (Five Whys, Root Cause Analysis)
- Why resistance happens and what to do about it
- Positive approaches: from diagnosis to possibilities

**Close Part 3**
