## 4. Targets & business case

:::video
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/zFj9J3Yeir8?rel=0&modestbranding=1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<a href="presentations/03-targets-and-sustainability.pdf" class="download-btn">
  <svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M21 15v4a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2v-4"/><polyline points="7 10 12 15 17 10"/><line x1="12" y1="15" x2="12" y2="3"/></svg>
  Download slides (PDF)
</a>

Part 3: Targets and sustainability
- The 70% myth debunked
- Setting meaningful targets (and Goodhart's Law)
- Sustaining change after consultants leave
:::

How do we know if implementation succeeds? What targets should we set? And how do we design implementations that produce lasting change?

### The 70% myth debunked

You may have heard the following claim:

> "The brutal fact is that about 70% of all change initiatives fail."
> â€” Beer & Nohria [-@beernohria2000]

:::summary
**The 70% myth:** Hughes [-@hughes2011] found this widely cited statistic has no valid empirical foundation.
:::

The 70% figure cannot be traced to any rigorous study. Most citations are circular, citing other citations rather than primary research. Higher failure rates tend to come from consulting firms (who benefit from the narrative), and "failure" is rarely defined. Actual evidence suggests outcomes vary enormously by type of change.

Why does the myth persist? Hughes suggests several functions:

| Function | How the myth serves it |
|----------|------------------------|
| **Legitimation** | Justifies consultant involvement in change |
| **Expectation setting** | Prepares clients for difficulties |
| **Risk management** | Consultants protected if things go wrong |
| **Selling** | Creates demand for change management expertise |

The danger: if people believe 70% of changes fail, this may become a self-fulfilling prophecy. The "nocebo effect" (expecting negative outcomes) can contribute to them.

:::framework
**A more evidence-based perspective:**
- Success and failure depend heavily on what you're trying to change
- Most changes produce mixed results (neither complete success nor failure)
- "Success" requires clear definition before it can be measured
- Organisational context matters more than methodology
:::

:::summary
**Business cases and targets:** Every implementation needs a business case, and meaningful targets to measure success. But Goodhart's Law warns that when a measure becomes a target, it ceases to be a good measure.
:::

A business case answers: why should the organisation invest resources in this change? It covers the current state and cost of inaction, the proposed change, expected benefits, costs, risks, and timeline. For consulting engagements, it must also justify consultant fees, client staff time, disruption during transition, and ongoing costs to sustain the change.

Targets focus attention and enable measurement, but poorly designed targets create problems. **Goodhart's Law** describes how targets can corrupt the behaviour they're meant to improve: hospital waiting time targets led to patients held in ambulances (not yet "waiting"); call centre duration targets led to calls ended prematurely without solving problems.

:::framework
**Designing better targets:**

| Principle | Application |
|-----------|-------------|
| **Multiple measures** | Balance different aspects of performance |
| **Leading and lagging** | Track inputs/activities AND outcomes |
| **Qualitative alongside quantitative** | Numbers plus narrative |
| **Beware perverse incentives** | Consider how targets could be "gamed" |
| **Review and adapt** | Targets should evolve as understanding develops |
:::

SMART targets (Specific, Measurable, Achievable, Relevant, Time-bound) are a useful starting point but not sufficient. A target can be SMART and still be harmful if it creates perverse incentives. Manheim [-@manheim2023] provides a systematic framework for building metrics that resist gaming, including strategies such as diversifying measures, randomising which targets are assessed, and incorporating qualitative judgement.

<!-- Summary based on full-text article via PMC open access (verified 2026-02-09) -->
:::source[manheim2023]
Manheim [-@manheim2023] provides a systematic framework for understanding why metrics fail and how to design better ones. Drawing on Campbell's Law and Goodhart's Law, the paper argues that naive metric implementation routinely distorts the systems it aims to measure, as participants optimise for the measure rather than the underlying goal.

**Key findings**:
- Metrics fail through four modes: difficulty finding task-appropriate data, imperfect correlations between measures and goals, perverse incentives arising from misused correlations, and incoherent or poorly articulated goals.
- Seven desirable properties of metrics (cost, availability, immediacy, simplicity, fairness, trust, and non-corruptibility) frequently conflict, requiring deliberate trade-offs rather than optimisation on a single dimension.
- Mitigation strategies include diversifying metrics, using secret or post hoc specified measures, randomising which metrics are assessed, incorporating qualitative "soft" judgements, and targeting satisfactory thresholds rather than maximisation.
- A six-step design process is proposed: understand the system, identify goals, select desiderata, brainstorm measures, plan implementation with pre-gaming exercises, and schedule regular reviews.

**Key insight**: Every metric that becomes a target risks becoming a bad metric; consultants must design measurement systems that anticipate and resist gaming through diversification, secrecy, and structured compromise.
:::

### Positive approaches to implementation

[Week 3](../W3_data_diagnosis/#part4) introduced several approaches that shift focus from "problems" to "possibilities". During implementation, these approaches become methods for **sustaining** change, not just diagnosing situations.

#### Positive Deviance in implementation

Pascale and Sternin's [-@pascalesternin2005] positive deviance approach (introduced in Week 3 for diagnosis) provides a powerful implementation model. Rather than implementing external solutions, positive deviance finds and amplifies solutions that already exist within the community.

**Implementation application:**
- Identify early adopters who've made the change work
- Create opportunities for them to share their approaches
- Enable peer learning and adaptation
- Spread success through social networks rather than mandates

The key shift: from "rolling out" a solution to "amplifying" existing successful practices.

#### Appreciative Inquiry's 5D cycle

Cooperrider's [-@cooperrider1987] Appreciative Inquiry, also introduced in Week 3, offers a full cycle that extends through implementation:

| Phase | Focus | Implementation role |
|-------|-------|---------------------|
| **Define** | What is the focus of inquiry? | Scoping the change |
| **Discover** | What's working well? | Finding strengths to build on |
| **Dream** | What could be? | Visioning desired future |
| **Design** | What should be? | Co-creating new approaches |
| **Destiny** | How to sustain? | Implementation and ongoing development |

The **Destiny** phase is specifically about implementation and sustainability: making dreams and designs real through action while continuously learning and adapting.

#### Action Learning

Revans [-@revans1980] developed action learning as an approach that combines real-world problem-solving with learning.

:::framework
**Action Learning principles:**

- Learning happens through working on real problems (not simulations)
- Small groups ("sets") support each other's learning
- Questions are as important as answers
- Reflection is built into action cycles
- "Comrades in adversity" provide challenge and support
:::

For implementation, action learning means:
- Implementation teams learn while implementing
- Regular reflection on what's working and what isn't
- Peer support across different parts of the change effort
- Problems become learning opportunities, not just obstacles

#### Whole System approaches

Weisbord's [-@weisbord1987] "whole system" approach (covered in Week 3's data sources section) extends into implementation through methods like Future Search that bring large groups together to create shared commitment to change.

**Implementation principles from whole system thinking:**
- Get the "whole system in the room" for key decisions
- Enable people to work on problems they care about
- Create shared understanding before seeking agreement
- Build commitment through participation, not persuasion

### Sustaining change

:::summary
**Sustaining change:** The ultimate test of implementation is whether the change persists after consultants leave.
:::

Multiple threats can erode change over time: key champions leave, competing priorities emerge, initial enthusiasm fades, systems revert to familiar patterns, new leadership has different priorities, and maintenance is less exciting than creation.

**Sustainability strategies:**

| Strategy | Approach |
|----------|----------|
| **Embed in systems** | Integrate change into processes, policies, technology |
| **Develop capability** | Build skills so change doesn't depend on specific people |
| **Align incentives** | Ensure reward systems support new behaviours |
| **Maintain visibility** | Keep measuring and reporting on progress |
| **Create communities** | Connect people working with the new approaches |
| **Plan handover** | Explicit transfer of responsibility to internal owners |

:::reflect
Think about a change you've experienced that didn't sustain. What factors contributed to the erosion? What might have helped it persist?
:::

### The transition from change agent to capability builder

As implementation progresses, the consultant's role shifts from driving change to building client capability, moving from Doctor or Plumber towards Navigator and Advisor, as discussed in [Section 1](#part1).

The transition is complete when:
- Internal teams can solve problems independently
- Systems support the new ways of working
- Leaders are visibly sustaining the change
- The consultant's absence wouldn't reverse progress

This is success, even if it means the consultant is no longer needed.
