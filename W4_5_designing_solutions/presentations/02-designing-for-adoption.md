# Recording 2: Designing for Adoption

**Duration:** ~15 minutes
**Slides:** 1-15
**Core message:** Effective solutions are designed with adoption in mind from the start. Good design isn't just analytical rigour—it's anticipating how real people will experience the change.

---

## Slide 1: Title

**Visual:** Title slide with module name

> "Designing for Adoption"

**Story to tell:**
Here's a question worth sitting with: what makes a solution "good"?

The obvious answer is analytical quality—thorough diagnosis, sound logic, evidence-based recommendations. These matter enormously. But there's another dimension that separates recommendations that change things from recommendations that gather dust: *adoptability*.

A solution designed for adoption isn't just strategically correct. It anticipates how people will experience the change, what resistance it will encounter, and what conditions are needed for success. This isn't about compromising on quality—it's about expanding what quality means.

In this recording, we'll explore how to design solutions that are built for the real world from the start.

---

## Slide 2: Microsoft's $7 Billion Lesson

**Visual:** Timeline showing 2013 acquisition to 2016 write-off

**Story to tell:**
Let me tell you about one of the most expensive lessons in solution design.

In September 2013, Microsoft announced it would acquire Nokia's mobile phone division for $7.2 billion. The strategic logic was compelling: Microsoft needed mobile hardware to compete with Apple and Google. Nokia had world-class manufacturing and brand recognition. Together, they could challenge the smartphone duopoly.

The deal closed in 2014. By 2016, Microsoft had written off nearly the entire acquisition and sold what remained for $350 million—a 95% loss.

What went wrong? It wasn't the strategy. The analysis that mobile mattered was correct. The problem was designing a solution that couldn't actually be adopted—by the organisations that needed to implement it.

**Key insight:**
Strategic logic without adoption design isn't strategy. It's expensive theory.

---

## Slide 3: When Cultures Collide

**Visual:** Two columns contrasting Nokia and Microsoft cultures

**Story to tell:**
The Microsoft-Nokia integration failed partly because nobody designed for the cultural reality.

Nokia was Finnish—egalitarian, consensus-driven, with flat hierarchies and collaborative decision-making. Microsoft was American—individualistic, competitive, with aggressive performance metrics and top-down direction.

Finnish employees found Microsoft's communication style abrasive and confrontational. Microsoft employees found Nokia's approach frustratingly slow and passive. Neither was wrong—they were just different.

But here's what matters for solution design: these differences were knowable in advance. Culture isn't hidden. It's visible in how people interact, what they reward, how decisions get made. Due diligence could have surfaced this. The solution design could have anticipated it.

Instead, cultural integration was treated as an afterthought—something to sort out after the deal closed. By then, trust had eroded, talent was leaving, and the solution was already failing.

**Key insight:**
Solutions need to go beyond the strategic level to encompass the cultural and practical. Design for the organisation that exists, not the one you wish existed.

---

## Slide 4: The Armenakis Design Checklist

**Visual:** Table showing the five beliefs as design questions

**Story to tell:**
Armenakis and colleagues identified five beliefs that determine whether people support or resist change. Most people encounter these as an implementation framework—something to manage *after* designing the solution.

But I want you to think about them differently: as a design checklist. Before you finalise any recommendation, ask:

*Discrepancy*: Will people believe change is necessary? If your solution addresses a problem they don't see, you have a design flaw.

*Appropriateness*: Will they believe this is the right change? Even if they accept something needs to change, they may think your specific solution is wrong.

*Efficacy*: Will they believe they can actually do it? A technically brilliant solution that exceeds people's capabilities isn't brilliant—it's fantasy.

*Principal support*: Will they believe leaders are committed? If your solution requires sustained attention from leaders who won't provide it, redesign the solution.

*Valence*: What's in it for them? Self-interest isn't shameful—it's human. If your recommendation makes key people's lives worse, expect resistance.

**Key insight:**
Use these as design criteria, not just implementation tactics. A solution that fails on any belief should be redesigned before it's presented.

---

## Slide 5: The Case for Multiple Options

**Visual:** Three diverging arrows, then converging to one

**Story to tell:**
Here's a common consulting failure mode: you find the "right" answer and present it.

This feels efficient. Why waste time on inferior alternatives? But it creates problems.

First, it looks like you're telling the client what to do. Even if you're right, this undermines ownership. The client becomes a critic evaluating your idea rather than a co-creator invested in the solution.

Second, you might be wrong. Confirmation bias is real. Once you've identified your preferred option, you'll unconsciously filter evidence to support it.

Third, you miss the learning. Developing multiple options forces you to articulate trade-offs, surface assumptions, and think through implications you'd otherwise skip.

The discipline of generating three or more options before evaluating them is fundamental to good design. First diverge—explore possibilities broadly. Then converge—evaluate systematically against criteria.

**Key insight:**
Options papers aren't about offering choice for its own sake. They're about ensuring rigorous thinking and building client ownership.

---

## Slide 6: Design Thinking for Consultants

**Visual:** Stanford d.school's five stages: Empathise, Define, Ideate, Prototype, Test

**Story to tell:**
Design thinking emerged from product design but applies directly to consulting.

The core insight: start with empathy. Before solving, understand deeply what people actually experience—not what they say they experience, not what management thinks they experience, but their lived reality.

The five stages provide a useful structure:

*Empathise*: Observe and engage with the people affected. What matters to them? What frustrates them? What would success look like from their perspective?

*Define*: Frame the problem clearly. A well-defined problem is half-solved. A poorly-defined problem leads to brilliant solutions to the wrong question.

*Ideate*: Generate many possible solutions. Quantity before quality. Suspend judgement. Build on others' ideas.

*Prototype*: Create rough versions quickly. The point isn't perfection—it's learning. What works? What doesn't?

*Test*: Get feedback from real users. Iterate. Refine.

This isn't a linear process—you'll cycle back repeatedly. But the discipline of empathising before defining, and ideating before prototyping, prevents the common trap of leaping to solutions.

**Key insight:**
Design thinking is a mindset: human-centred, iterative, prototype-driven. It treats early failure as learning, not defeat.

---

## Slide 7: Creative Problem-Solving

**Visual:** Comparison of traditional brainstorming vs brainwriting

**Story to tell:**
Let's talk about generating options. The obvious tool is brainstorming—get people in a room, generate ideas freely, build on each other's contributions.

The problem: research shows traditional brainstorming often underperforms individuals working alone. Why? Dominant personalities take over. People hold back to avoid seeming foolish. You can only voice one idea at a time, creating bottlenecks.

Paulus and Yang found that *brainwriting*—where people write ideas silently, then share them for others to build upon—often outperforms verbal brainstorming. It removes the social barriers while keeping the benefits of cross-pollination.

Other approaches worth knowing:

*Lateral thinking*: Deliberately break patterns. What if we did the opposite? What would a completely different industry do?

*Analogical reasoning*: How have similar problems been solved elsewhere? What can we borrow?

*Constraint manipulation*: What if we had unlimited budget? What if we had zero budget? What if we had to deliver in a week?

**Key insight:**
The goal isn't one creative technique—it's ensuring you've genuinely explored the solution space before narrowing down.

---

## Slide 8: Evaluating Options Systematically

**Visual:** Decision matrix with weighted criteria

**Story to tell:**
Once you have options, you need to evaluate them. Intuition has a role—but intuition alone is vulnerable to bias, anchoring, and post-hoc rationalisation.

A decision matrix brings rigour. The basic structure:

First, define your criteria. What does a good solution need to achieve? Include both hard requirements (cost, timeline, technical feasibility) and soft ones (cultural fit, stakeholder acceptance, reversibility).

Second, weight the criteria. Not everything matters equally. A solution that's cheap but culturally impossible is worse than one that's expensive but implementable.

Third, score each option against each criterion. Be honest—don't torture the data to reach your preferred conclusion.

Fourth, examine the results. The matrix doesn't make the decision—you do. But it makes your reasoning transparent and challengeable.

**Key insight:**
The value isn't the final score—it's the conversation the matrix enables. Making trade-offs explicit builds shared understanding.

---

## Slide 9: Scenario Planning

**Visual:** Multiple future pathways branching from the present

**Story to tell:**
Solutions operate in uncertain futures. A recommendation that works brilliantly if the economy grows may be disastrous if it contracts. A technology choice that looks smart today may look foolish if the market shifts.

Scenario planning helps design solutions that are robust across multiple plausible futures.

The approach: identify the key uncertainties that would most affect your recommendation. Develop two to four distinct scenarios—not best/worst case, but genuinely different futures. Then stress-test your solution against each scenario.

The question isn't "which future is most likely?" Prediction is usually impossible. The question is: "How does our solution perform across these different futures? Where is it vulnerable? Can we design in flexibility?"

Sometimes this reveals that a recommendation is too fragile—it only works if specific conditions hold. That's valuable learning. Better to discover fragility in planning than in implementation.

**Key insight:**
Good solutions aren't optimised for one predicted future—they're designed to work reasonably well across multiple possible futures.

---

## Slide 10: The Pre-Mortem

**Visual:** Project timeline with "failure point" marked, working backwards

**Story to tell:**
Gary Klein developed a technique that turns our cognitive biases into an advantage.

In a pre-mortem, you ask the team to imagine that the project has failed spectacularly—then generate plausible reasons why.

The magic: by framing failure as already having happened, you unlock insights that optimism bias normally suppresses. People who have reservations can voice them as explanations rather than objections. "This failed because..." is easier to say than "This might fail because..."

Research shows that prospective hindsight—imagining an event has already occurred—increases the ability to identify reasons for outcomes by 30%.

Run a pre-mortem before finalising any significant recommendation. What might go wrong? What are we assuming that might not hold? What could derail this? Then use the insights to strengthen your design.

**Key insight:**
The pre-mortem surfaces the concerns people have but aren't voicing. It's easier to explain a past failure than to predict a future one.

---

## Slide 11: Designing Within Constraints

**Visual:** Venn diagram showing client context, solution quality, implementation feasibility

**Story to tell:**
Let's be honest about something: consultants sometimes design solutions that can't actually work.

Not because they're analytically wrong—because they ignore constraints. The client doesn't have the budget. The culture won't support it. The timeline is impossible. Key stakeholders will never agree.

These aren't implementation problems to solve later. They're design parameters to respect from the start.

Designing within constraints isn't about lowering ambition. It's about directing ambition usefully. The question shifts from "What would be ideal?" to "What's the best achievable outcome given reality?"

Sometimes this means phased approaches—what's possible now, what becomes possible once early wins build credibility. Sometimes it means smaller scope but higher confidence. Sometimes it means the constraint itself is the problem to address first.

**Key insight:**
Implementation feasibility shapes design. A brilliant recommendation that can't be implemented isn't brilliant—it's useless.

---

## Slide 12: The Options Paper

**Visual:** Structure showing recommendation format with options

**Story to tell:**
How you present recommendations affects whether they're adopted. The "options paper" format structures recommendations as choices rather than mandates.

The basic structure:
- Context: What's the situation? What problem are we solving?
- Options: Three or more genuine alternatives, each with trade-offs honestly presented
- Evaluation: How do options compare against agreed criteria?
- Recommendation: Which option do you recommend, and why?

Notice: you still make a recommendation. You're not abdicating judgement. But you're inviting the client into a decision rather than asking them to accept or reject your verdict.

This builds ownership. The client isn't defending your idea—they're choosing among alternatives they helped evaluate. That psychological shift matters enormously for implementation.

**Key insight:**
Structuring recommendations as choices positions you as a thinking partner, not an oracle issuing pronouncements.

---

## Slide 13: Prototyping and Testing

**Visual:** Cycle showing prototype → test → learn → iterate

**Story to tell:**
Not every solution needs to be implemented fully from day one. Prototyping and pilot testing reduce risk and build evidence.

A prototype is a rough, quick version designed for learning. What's the minimum you need to build to test your core assumptions? Can you simulate the solution? Try it in one team? Run a tabletop exercise?

The point isn't to prove the solution works—it's to discover what doesn't work before committing fully.

Pilot testing takes this further: implement in one area before rolling out widely. This generates evidence, surfaces unexpected issues, and creates advocates who've experienced success firsthand.

Both approaches treat solutions as hypotheses to be tested rather than truths to be installed. They build in learning cycles—mechanisms to adjust based on what you discover.

**Key insight:**
Solutions designed for learning are more adaptable than solutions designed for perfection. Build in mechanisms to test and adjust.

---

## Slide 14: The Nodding Problem

**Visual:** Meeting room with thought bubbles showing unspoken concerns

**Story to tell:**
Let me return to something crucial for adoption: nodding isn't commitment.

When stakeholders nod during your presentation, they might be agreeing. Or they might be being polite, waiting for the meeting to end, reserving judgement, or quietly planning how to work around your recommendation.

The moment of truth isn't the presentation. It's what happens afterwards—when people face competing priorities, institutional inertia, and the gravitational pull of how things have always been done.

This is why design for adoption matters so much. A solution that surfaces concerns early, builds genuine ownership, respects constraints, and anticipates resistance has a far better chance than one that's merely analytically correct.

**Key insight:**
Success isn't measured by approval in the feedback meeting. It's measured by what changes in the organisation six months later.

---

## Slide 15: Bridge to Next Recording

**Visual:** Preview of "The Evidence Trap"

**Story to tell:**
We've talked about designing solutions for adoption. But there's a prior question: how do you know what evidence to base your solutions on?

Evidence-based practice sounds like the obvious answer. But it's more complicated than it seems. Evidence can illuminate—but it can also obscure. It can inform decisions—or be weaponised to justify decisions already made.

Next recording: we'll explore the promise and the pitfalls of evidence-based consulting.

**Reflection prompt:** Think about a solution—at work or elsewhere—that was technically good but failed to be adopted. What design flaws made it unsuitable for the people and context it was meant to serve?

---

## Sources

- Armenakis, A. A., Harris, S. G., & Mossholder, K. W. (1993). Creating readiness for organizational change. *Human Relations*, 46(6), 681-703
- Brown, T. (2008). Design thinking. *Harvard Business Review*, 86(6), 84-92
- Klein, G. (2007). Performing a project premortem. *Harvard Business Review*, 85(9), 18-19
- Microsoft-Nokia acquisition: Various business press analyses (2013-2016); Success Across Cultures case study
- Paulus, P. B., & Yang, H. C. (2000). Idea generation in groups: A basis for creativity in organizations. *Organizational Behavior and Human Decision Processes*, 82(1), 76-87
- Schoemaker, P. J. H. (1995). Scenario planning: A tool for strategic thinking. *Sloan Management Review*, 36(2), 25-40
