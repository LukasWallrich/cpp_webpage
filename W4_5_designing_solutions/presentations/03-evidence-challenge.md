# Recording 3: The Evidence Challenge

**Duration:** ~12 minutes
**Slides:** 1-15
**Core message:** Evidence-based practice is valuable but contested. The real skill is knowing when evidence helps, when it obscures, and when other considerations legitimately take precedence.

---

## Slide 1: Title

**Visual:** Title slide with module name

> "The Evidence Challenge"

**Story to tell:**
Two stories to start.

Story one: A retail company launches a major customer service initiative based on leadership intuition about what customers want. They spend millions. It fails. Later analysis shows all the warning signs were in existing data—data nobody looked at.

Story two: A hospital implements an evidence-based scheduling system that maximises operating theatre utilisation. The metrics look perfect. But staff burn out, patient satisfaction drops, and errors increase. The evidence supported an intervention that made things worse.

Evidence matters. And evidence can mislead. That tension is what this recording is about.

---

## Slide 2: The Case FOR Evidence

**Visual:** Evidence pyramid with meta-analyses at top

**Story to tell:**
Let's start with the positive case. Why should consultants care about evidence?

First, accumulated research often reveals patterns invisible to any individual case. Systematic reviews synthesise dozens or hundreds of studies. If something consistently works—or doesn't—evidence tells us.

Second, evidence provides defensibility. When a client challenges your recommendation, "This aligns with research across 50 organisations" is more compelling than "I've seen this work before."

Third, evidence disciplines our thinking. It forces us to ask: how do we know this? What would change our mind? Where might we be wrong?

The evidence hierarchy—from case studies at the bottom to meta-analyses at the top—helps us assess what we actually know.

**Key insight:**
Evidence isn't about being scientific for its own sake. It's about reducing the odds of being wrong.

---

## Slide 3: The Six As of Evidence-Based Practice

**Visual:** Six steps: Ask, Acquire, Appraise, Aggregate, Apply, Assess

**Story to tell:**
Briner offers a practical framework for applying evidence: the Six As.

*Ask*: Start with a clear question. Not "how do we improve engagement?" but "what interventions demonstrably increase engagement in professional services contexts?"

*Acquire*: Search systematically. Don't stop at the first confirming study.

*Appraise*: Critically evaluate what you find. Sample size? Context? Methodology?

*Aggregate*: Combine evidence from multiple sources. What's the overall picture?

*Apply*: Integrate evidence with expertise and context. Evidence rarely gives you the answer—it informs judgment.

*Assess*: Evaluate outcomes. Did the evidence-based approach actually work here?

**Key insight:**
Evidence-based practice isn't "do what studies say." It's a disciplined process of inquiry that improves decision quality.

**Go deeper:** The written content provides detailed guidance on applying each step.

---

## Slide 4: The Four Sources of Evidence

**Visual:** Four quadrants diagram showing: Stakeholder Views, Practitioner Expertise, Organisational Data, Scientific Literature

**Story to tell:**
The Six As tell you *how* to work with evidence. But evidence of what? Briner argues we need to draw on four distinct sources—not just research.

*Stakeholders' views, perspectives, and judgments*: What do the people affected by this decision value and fear? Their knowledge of local context is evidence.

*Professional expertise of practitioners*: What have experienced people learned about what works here? Practical wisdom counts.

*Organisational data*: What do engagement surveys, turnover rates, performance metrics, and financial data tell us? Internal evidence often trumps external studies.

*Scientific literature*: What does rigorous research show about interventions like this?

The trap is treating "evidence-based" as meaning only scientific literature. Good practice integrates all four.

**Key insight:**
Evidence-based practice doesn't mean "do what studies say." It means systematically considering multiple sources of evidence—including stakeholder views and practitioner judgment.

---

## Slide 5: When Evidence Gets Ignored

**Visual:** Split screen: Scared Straight program / Open-plan office

**Story to tell:**
Let me show you a troubling pattern—cases where strong evidence existed but was ignored.

*Scared Straight programs*: These programs—where at-risk youth visit prisons to be confronted by inmates—were enormously popular from the 1970s onwards. Politicians loved them. The intuitive logic seemed obvious. But systematic reviews showed they actually *increased* reoffending rates. The Department of Justice formally recommended avoiding them. Yet the programs continued for decades because they *felt* right.

*Open-plan offices*: Research since the 1970s has consistently shown they harm productivity. A major study found enclosed offices clearly outperformed open plans. Another study found face-to-face interactions actually *dropped* 70% when companies switched to open layouts—the opposite of what proponents claimed. Yet companies continue implementing them. Why? Cost savings. And a narrative about collaboration that feels intuitively true.

The pattern: when evidence contradicts deeply held beliefs or financial interests, evidence often loses.

**Key insight:**
Intuitive appeal and financial incentives can trump even robust empirical evidence. Being evidence-based means checking whether your beliefs align with data—not just citing data that confirms your beliefs.

---

## Slide 6: Internal Evidence Ignored

**Visual:** Three company logos with tombstones: Kodak, Nokia, Blockbuster

**Story to tell:**
Consider three cautionary tales from business history:

Kodak invented the first digital camera in 1975. When engineer Steve Sasson presented it, management's response was—by his own account—"curiosity and skepticism." They saw it as "a very scary look at what could be possible" rather than an opportunity. Kodak filed for bankruptcy in 2012.

Nokia's internal analysts recognised the iPhone threat immediately. The day after Apple's 2007 unveiling, a nine-person team wrote a presentation warning that the iPhone's touchscreen interface could become "a new standard" and recommending Nokia develop its own touch interface to "fight back." Leadership didn't act. By 2013, Nokia sold its phone division to Microsoft.

Blockbuster turned down Netflix for $50 million in 2000, dismissing it as a "very small niche business." Netflix is now worth over $150 billion.

In each case, internal evidence pointed to the future. Leaders ignored it.

**Key insight:**
Evidence can be uncomfortable. Organisations develop immune systems that reject inconvenient truths—especially ones that threaten existing business models.

---

## Slide 7: The Bandwagon Effect

**Visual:** Arrows converging (mimetic isomorphism diagram) or dominoes falling in sequence

**Story to tell:**
Here's the opposite problem: following evidence *too* uncritically.

When research shows something works, organisations often adopt it wholesale—regardless of fit. Institutional theorists call this *isomorphism*: organisations in similar fields becoming increasingly similar, not because the practices work but because adopting them seems legitimate.

DiMaggio and Powell identified three drivers. *Mimetic* isomorphism: under uncertainty, organisations copy what successful peers do. *Normative* isomorphism: professional training and networks spread "best practices." *Coercive* isomorphism: regulations and powerful stakeholders mandate compliance.

Consider management fashions. Balanced Scorecards. Six Sigma. Agile. Each backed by evidence of success somewhere. Each adopted by organisations where it didn't fit. The evidence became a bandwagon rather than a tool for judgment.

The risk isn't just that organisations ignore evidence. It's that they follow it too literally, treating contextual findings as universal truths.

**Key insight:**
"The research supports it" can become its own form of groupthink. Evidence should inform judgment, not replace it.

---

## Slide 8: Whose Success? What Timeframe?

**Visual:** Two axes: Whose benefit (stakeholder groups) × Timeframe (short/long-term)

**Story to tell:**
When evaluating evidence, ask: whose outcomes count?

An intervention might show positive results for shareholders while harming workers. Efficiency metrics might improve while wellbeing declines. Short-term gains might create long-term fragility.

Evidence is never neutral in what it measures. The choice of outcome variables embeds values. "Success" according to whom?

And timeframe matters. An intervention that looks brilliant at 6 months might fail at 3 years. Much management research captures only short-term effects—yet consultants recommend solutions that clients will live with for decades.

There's also the question of who produces the evidence. Pharmaceutical companies fund studies that tend to favour their drugs. Consultancies fund research that validates their methodologies. This doesn't mean all funded research is corrupt—but funding sources create systematic biases.

**Key insight:**
Ask not just "does the evidence support this?" but "what does this evidence actually measure, for whom, and over what timeframe?"

---

## Slide 9: Policy-Based Evidence

**Visual:** Two directions: evidence-based policy vs. policy-based evidence

**Story to tell:**
There's a cynical phrase in policy circles: "policy-based evidence." It describes what happens when leaders decide what they want to do, then cherry-pick research to justify it.

Surveys of managers consistently find that most make decisions based primarily on personal experience. Research articles rarely get read. This isn't surprising—managers are busy, research can be inaccessible, and experience feels more immediate than statistics.

But it means we have a tendency to cherry-pick research that backs up a perspective and ignore research that does not.

Consultants can be complicit in this. "Policy-based evidence" is a risk for consulting recommendations too.

**Key insight:**
Using evidence selectively isn't evidence-based practice. It's marketing with academic citations.

---

## Slide 10: The Case AGAINST Evidence-Based Management

**Visual:** Morrell & Learmonth quote about narrowness

**Story to tell:**
Not everyone is convinced that evidence-based management is the answer. Morrell and Learmonth mount a forceful challenge.

Their arguments: Evidence-based management risks *narrowness*—privileging what can be measured over what matters. It promotes a *positivist* outlook that misses important qualitative dimensions. And the *medical metaphor* may be fundamentally misapplied—organisations are more varied, contested, and political than medical contexts.

Their alternative: intellectual *pluralism* and *openness*. What they call "management learning" rather than evidence-based management—recognising that different situations require different ways of knowing.

**Key insight:**
Evidence-based practice can become its own ideology—one that dismisses legitimate concerns that don't fit into quantitative frameworks.

**Go deeper:** The written content explores this critique in depth, with the full Morrell & Learmonth source block.

---

## Slide 11: The Black Report Problem

**Visual:** UK government filing report

**Story to tell:**
Here's a historical example that reveals something important about how evidence interacts with politics. The Black Report, published in 1980, provided compelling evidence of health inequalities in the UK and recommended measures to address them.

The Conservative government received it—and effectively buried it. Why? Because the recommendations didn't fit their political ideology.

As one analysis put it: it is the existing political ideology which determines the likelihood of acceptance of evidence rather than its mere scientific quality.

This isn't a critique of evidence. It's a recognition that evidence operates in political contexts. Technical quality doesn't guarantee uptake.

**Key insight:**
Evidence is necessary but not sufficient. Political will and ideological alignment shape whether evidence leads to action.

---

## Slide 12: Context Matters

**Visual:** Same intervention, different outcomes in different settings

**Story to tell:**
One of the deepest challenges for evidence-based practice: context dependence. Something that works in one setting may fail in another.

Consider: an intervention tested in large American corporations may not translate to UK SMEs, or public sector organisations, or different national cultures. The research shows what happened *there*. Your context is *here*.

This is why "aggregate" in the Six As matters—triangulating multiple sources helps. But even systematic reviews often can't tell you whether an intervention will work in your specific situation.

This is also why Briner's Four Sources matter. Scientific literature tells you what worked elsewhere. Organisational data tells you what's happening here. Practitioner expertise tells you what's been tried. Stakeholder views tell you what will be accepted.

Good evidence use requires judgment: how similar is this context to contexts where the evidence was generated?

**Key insight:**
Evidence informs judgment; it doesn't replace it. Knowing when to generalise and when to contextualise is a skill.

---

## Slide 13: Holding the Tension

**Visual:** Balance scale with evidence and other considerations

**Story to tell:**
So where does this leave us? In a productive tension.

Evidence-based practice offers real value:
- Reduces reliance on gut feeling and fashion
- Provides defensible basis for recommendations
- Forces explicit consideration of what we know and don't know

But it has real limitations:
- Not everything important is easily measurable
- Context shapes whether findings apply
- Values and politics legitimately shape decisions alongside evidence

The best consultants hold both: committed to evidence while recognising its limits. Rigorous AND humble.

**Key insight:**
"What does the research say?" is a good question. It's not the only question.

---

## Slide 14: A Practical Stance

**Visual:** Framework: Four Sources + contextual judgment + ethical awareness

**Story to tell:**
Let me suggest a practical stance. For any recommendation:

First, consult all four sources. What does scientific research say? What does your organisational data show? What do experienced practitioners know? What do stakeholders believe and value?

Second, exercise judgment. Given this client's specific situation, constraints, and culture, does the evidence apply? What contextual factors might matter?

Third, maintain ethical awareness. Whose interests does this recommendation serve? What values are embedded in how success is measured? What might be overlooked?

This isn't a formula. It's a disposition—combining empirical rigour with practical wisdom.

**Key insight:**
Evidence-based practice at its best isn't mechanical application of research findings. It's disciplined inquiry that improves decision quality while remaining humble about uncertainty.

---

## Slide 15: Bridge to Next Recording

**Visual:** Preview of "Ethics Without Easy Answers"

**Story to tell:**
We've talked about solutions, buy-in, and evidence. There's one more dimension we need to address: ethics.

Consulting has no external regulator. Nobody can strike you off. Nobody certifies your competence. This freedom comes with responsibility—you must develop your own ethical framework.

But institutional structures often make individual ethics difficult to maintain. Conflicts of interest are structural, not personal. And sometimes the ethical choice is to walk away.

Next recording: ethics without easy answers.

**Reflection prompt:** Think about evidence that informed a decision you made recently—at work or in life. What other factors mattered alongside that evidence? In retrospect, did you weigh them appropriately?

---

## Sources

- Briner, R. (2009). The basics of evidence-based practice. *People & Strategy*
- Briner, R., Denyer, D., & Rousseau, D. M. (2009). Evidence-based management: Concept cleanup time? *Academy of Management Perspectives*, *23*(4), 19-32
- Morrell, K. & Learmonth, M. (2015). Against evidence-based management. *Academy of Management Learning & Education*
- DiMaggio, P. J. & Powell, W. W. (1983). The iron cage revisited: Institutional isomorphism and collective rationality in organizational fields. *American Sociological Review*, *48*(2), 147-160
- Open-plan office research: Kim & de Dear (2013), *Journal of Environmental Psychology*; Bernstein & Turban (2018), *Philosophical Transactions of the Royal Society B*
- The Black Report: PMC historical analysis; Socialist Health Association archive
- CIPD evidence-based practice factsheet
- Kodak case: IEEE Spectrum (Steve Sasson interview); Snopes fact-check analysis
- Nokia case: INSEAD Knowledge case study
- Blockbuster/Netflix case: Fortune (Marc Randolph interview, 2023)
- Scared Straight research: Petrosino, A., Turpin-Petrosino, C., Hollis-Peel, M. E., & Lavenberg, J. G. (2013). Scared Straight and other juvenile awareness programs for preventing juvenile delinquency. *Campbell Systematic Reviews*, 9(1), 1-55
