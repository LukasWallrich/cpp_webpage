# Part 1: Opportunities and Challenges in Evaluation
**Target duration: ~15-20 minutes**

## Narrative Arc
This part establishes context (full circle in the consultancy cycle), introduces the fundamental tension between appearing successful and learning, explores why "success" depends entirely on who you ask, and considers why rigorous evaluation remains rare. The goal is to provoke thinking about evaluation as political, not just technical.

## Slide-by-Slide Outline

### Slide 1: Title Slide
**Content**: "Consultancy and Professional Practice - W7: Concluding Engagements & Evaluating Success"
- Brief welcome and overview
- "This week we complete the consultancy cycle—exploring how engagements end, how success is evaluated, and how learning is captured"

---

### Slide 2: The Consultancy Cycle
**Content**: The Consultancy Cycle diagram with concluding phase highlighted

**Figure**: Consultancy Cycle diagram with W7 section emphasised

**Story to tell**:
- We've come full circle—back to where we started
- This phase connects back to W2: what you contract determines how you can conclude
- Key insight: if you didn't agree success criteria at contracting, you'll struggle to demonstrate value now
- Disengagement planning starts early, not at the end

---

### Slide 3: This Week's Agenda
**Content**: Three-part structure

1. **Evaluation** opportunities and challenges (Part 1)
2. **Handover & Closure** planning for positive disengagement (Part 2)
3. **Learning & Packaging** capturing and commoditising knowledge (Part 3)

**Story to tell**:
- Three video recordings, around 60 minutes total
- Core readings: Block Chapter 21 (required for all), plus APM handover report or Heusinkveld & Benders
- Case study: Figgie International—we'll discuss the ethical dimensions in the live session

---

### Slide 4: Why Evaluation Is "Not Much Liked"
**Content**: The fundamental tension

**Quote**: "Organisations face conflicting needs—the need to appear successful versus the need to learn from mistakes" (Skinner, 2004)

**Story to tell**:
- Evaluation sounds obviously valuable—so why doesn't it happen more?
- Meaningful evaluation requires acknowledging what didn't work
- This threatens reputations, careers, and carefully constructed narratives of success
- The result: systemic bias toward superficial evaluation
- Note: Lovallo & Kahneman (2003) show that cognitive biases—especially optimism bias—mean projects are often over-promised from the start. Evaluation threatens to expose this.

---

### Slide 5: Success According to Whom?
**Content**: The perspective problem — evaluation results depend entirely on who you ask

**Figure**: McKinsey transformation success data (Maor, Reich & Yocarini, 2017)

| Role | Report success |
|------|---------------|
| CEO | 34% |
| PMO / transformation office leader | 24% |
| Senior leaders | 23% |
| Leaders of transformation initiatives | 13% |
| Line managers | 10% |
| Frontline employees | 6% |

*Overall average: 20% (n = 1,487)*

**Story to tell**:
- Look at the gradient: CEOs are almost six times more likely than frontline employees to call a transformation successful
- This isn't just about information asymmetry—these people experienced different realities
- Similarly, the MCA (2006) survey found that those who commission consultants report 48% satisfaction, but end-users report only 17%
- The question isn't "was the project successful?"—it's "successful according to whom?"
- This is why agreeing success criteria upfront matters—and whose criteria you use

---

### Slide 6: Competing Narratives
**Content**: Evaluation as political contest, not truth discovery

**Key argument** (Dawson & Buchanan, 2005):
- Organisational change produces multiple competing narratives
- Each serves different stakeholders' interests
- There is rarely one "true" story of what happened

**Story to tell**:
- Evaluation isn't just about discovering the truth—it's about which version of events becomes the accepted account
- Power dynamics determine whose narrative prevails
- Consider a hospital BPR project studied by Buchanan (2000): senior management called it "successful modernisation"; clinical staff saw "disruption to patient care with minimal benefits"; the consultants described "pioneering methodology with significant improvements"
- All contain elements of truth. The political question: whose story gets told in the final evaluation report?

---

### Slide 7: The Taboo of Failure
**Content**: Why organisations struggle to learn from evaluation

**Story to tell**:
- Thorne (2000): organisational cultures treat failure as something to hide, not learn from
- Tech culture claims to celebrate failure ("fail fast")—but even there, only failures that led to later success get celebrated
- If acknowledging failure is taboo, evaluation becomes performance art
- Consultant complicity: we also have incentives to claim success
- Easterby-Smith (1994) identified four purposes of evaluation: proving, improving, learning, controlling. In practice, "proving" dominates and crowds out the more valuable learning purposes

---

### Slide 8: The Evaluation Paradox
**Content**: Why the consulting industry rarely evaluates its own work

**Story to tell**:
- The paradox: an industry that advises on evidence-based decisions rarely evaluates its own interventions rigorously (Sturdy, 2011)
- Not because consultants are ineffective—because the conditions for rigorous evaluation rarely exist
- Clients rarely pay for it; attribution is genuinely hard; real impact takes years to materialise; nobody wants bad news
- Think about it: if you measured outcomes → impact, you'd need to wait months or years, control for other factors, and risk finding the answer is "we're not sure"
- Yet without evaluation, consulting operates partly on faith

---

### Slide 9: Part 1 Summary
**Content**: Recap and preview

**What we covered**:
- Full circle in the consultancy cycle
- The fundamental tension: appearing successful vs. learning
- Success depends on who you ask—perspective shapes evaluation dramatically
- Competing narratives make evaluation political, not just technical
- Practical constraints mean rigorous evaluation is rare

**The connecting thread**: Evaluation is inherently political and contested. Success depends on whose perspective prevails and what purposes the evaluation serves.

**Reflection**: Think about a project you've been involved with. How was success evaluated? Who defined the criteria? What perspectives were included or excluded?

**Preview of Part 2**: How do we plan for positive disengagement? What makes handovers succeed or fail?

**Close Part 1**

## Sources
- Buchanan, D. (2000). An eager and enduring embrace: the ongoing rediscovery of teamworking as a management idea. In S. Procter & F. Mueller (Eds.), *Teamworking*. Macmillan.
- Dawson, P. & Buchanan, D. (2005). The way it really happened: Competing narratives in the political process of technological change. *Human Relations*, 58(7).
- Easterby-Smith, M. (1994). *Evaluating Management Development, Training and Education* (2nd ed.). Gower.
- Lovallo, D. & Kahneman, D. (2003). Delusions of Success. *Harvard Business Review*, 81(7).
- Maor, D., Reich, A. & Yocarini, L. (2017). The people power of transformations. *McKinsey & Company*.
- Skinner, D. (2004). Evaluation and change management: rhetoric and reality. *Human Resource Management Journal*, 14(3).
- Sturdy, A. (2011). Consultancy's Consequences? *Journal of Management Studies*, 48(1).
- Thorne, M. L. (2000). Interpreting corporate transformation through failure. *Management Decision*, 38(5).
