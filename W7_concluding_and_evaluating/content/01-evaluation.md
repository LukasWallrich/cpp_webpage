## 1. Opportunities and challenges in evaluation

:::video
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/fWF2cNRpEI0?rel=0&modestbranding=1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<a href="presentations/01-evaluation.pdf" class="download-btn">
  <svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M21 15v4a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2v-4"/><polyline points="7 10 12 15 17 10"/><line x1="12" y1="15" x2="12" y2="3"/></svg>
  Download slides (PDF)
</a>

Part 1: Opportunities and challenges in evaluation
- Why evaluation is often avoided
- Success according to whom? The perspective problem
- Competing narratives and the political dimension
- The evaluation paradox
:::

### Why evaluation is "not much liked"

:::summary
Evaluation is essential in principle but often avoided in practice. As Skinner [-@skinner2004] notes, organisations have conflicting needs: the need to appear successful vs the need to learn from mistakes.
:::

This creates a fundamental barrier: **meaningful evaluation requires acknowledging what didn't work**. Lovallo & Kahneman [-@lovallo2003] show that cognitive biases, especially optimism bias, mean projects are often over-promised from the start, so evaluation threatens to expose uncomfortable truths.

Several factors explain why rigorous evaluation is rare in consulting:

**Political dimensions**: Evaluation findings enter a political arena where different stakeholders have competing interests in how the story is told. The consultant wants to demonstrate value; the client sponsor wants to show their decision was wise; affected staff may want to highlight problems.

**The taboo of failure**: Thorne [-@thorne2000] describes how organisational cultures often treat failure as something to be hidden rather than learned from. This contrasts sharply with technology culture, where Draper [-@draper2017] notes that "fail fast" has become a mantra, though even there, the reality is often more nuanced.

### Success and competing narratives

:::summary
Evaluation results depend on who you ask, and different stakeholders construct competing narratives about what happened. The question isn't "was the project successful?" but **"successful according to whom?"**
:::

A McKinsey survey [@maor2017] of nearly 1,500 people found that **CEOs report transformation success at 34%, while frontline employees report just 6%**. Similarly, the MCA (2006) found that those who commission consultants report 48% satisfaction vs 17% among end-users. This perspective problem means that success criteria need to be agreed upfront, and stakeholders need to be explicit about *whose* perspective matters most.

Dawson & Buchanan [-@dawsonbuchanan2005] argue that organisational change produces multiple competing narratives, each serving different stakeholders' interests. There is rarely one "true" story of what happened. They illustrate this with interviews at different levels of an Australian manufacturing company undergoing change:

| Level | Narrative |
|-------|-----------|
| Management | Positive team spirit, "heading towards being a world-class operation" |
| Supervisory | Communication failures: information not reaching the shop floor |
| Employee | Exploitation: "The managers get the limelight at the expense of the workers" |

Which narrative is "correct"? All contain elements of truth. The political question is: **whose story gets told in the final evaluation?** Evaluation isn't just about discovering the truth; it's about which version of events becomes the accepted account.

### The evaluation paradox

:::summary
The industry that advises others on evidence-based decisions rarely evaluates its own interventions rigorously. Sturdy [-@sturdy2011] notes that evidence for consulting's impact is remarkably thin, not because consultants are ineffective, but because the conditions for rigorous evaluation rarely exist.
:::

Clients rarely pay for evaluation; attribution is genuinely hard; real impact takes years to materialise; and nobody wants bad news. Yet without evaluation, consulting operates partly on faith.

<!-- Summary generated from PDF: sources/sturdy2011.pdf -->
:::source[sturdy2011]
*This is one of your core readings from Week 1.* Sturdy critically examines whether management consultancy actually impacts management practice, arguing that both critics and supporters tend to overstate its influence while also missing some of its more diffuse effects.

**Key findings/arguments**:
- Direct evidence of consulting impact is surprisingly thin. It's methodologically hard to isolate consultancy's effects from other influences on management
- Consultancy's impact is geographically concentrated: as of 2008, approximately 80% of worldwide fees came from just five nations (USA, Canada, UK, Germany, France), a figure that had declined to around 70% by 2015 (Sturdy & O'Mahoney, [-@sturdy2018])
- Consultants often serve as scapegoats for management decisions. It's easier to blame consultants than challenge management itself
- The boundaries between "consultant" and "manager" are increasingly blurred, making impact assessment even more difficult
- Evaluation is contested: those who commission consultants report much higher satisfaction (48%) than end-users (17%)

**Key insight**: We lack an adequate basis for grand claims about consulting's impact. Its influence has been both overstated (in geographical scope) and understated (in how consultancy practices have diffused into mainstream management).
:::

### Defining success criteria

The following sections expand on the video content with detailed frameworks for thinking about evaluation in practice.

:::framework
**Easterby-Smith's [-@easterbysmith1994] four purposes of evaluation**:

1. **Proving**: Demonstrating that an intervention worked (or didn't)
2. **Improving**: Learning how to do better next time
3. **Learning**: Gaining insight about the organisation and its dynamics
4. **Controlling**: Ensuring compliance with requirements or standards

These purposes can conflict. Proving success may discourage honest learning about failures.
:::

Most consulting evaluation defaults to **proving**: demonstrating ROI to justify the investment. But this is often the hardest purpose to achieve reliably, and it may crowd out the more valuable **improving** and **learning** purposes.

Think back to Week 1: why are consultants brought in? The answer shapes what counts as success:

| Reason for hiring | Success criteria |
|-------------------|------------------|
| Expertise/knowledge | Quality of recommendations, knowledge transferred |
| Capacity/resource | Work completed on time, within budget |
| Political cover | Decision supported, stakeholders aligned |
| Legitimacy | External validation, credibility with Board |
| Change catalyst | Momentum created, resistance addressed |

Different stakeholders may have hired the consultant for different reasons, and therefore have different success criteria:

- **Client sponsor**: Was my decision vindicated?
- **Client organisation**: Are we better off?
- **Consulting firm**: Can we claim success? Will they rehire us?
- **End users**: Has our work improved?
- **Shareholders**: Was this good value?

### Logic models for evaluation

:::framework
**Logic models** provide a framework for thinking through what you're evaluating:

**Inputs** → **Activities** → **Outputs** → **Outcomes** → **Impact**

- **Inputs**: Resources deployed (time, money, expertise)
- **Activities**: What the consultants did
- **Outputs**: Deliverables produced (reports, workshops, systems)
- **Outcomes**: Short-term changes (knowledge, attitudes, behaviours)
- **Impact**: Long-term effects on the organisation
:::

Most consulting evaluation focuses on **inputs** (did we stay within budget?), **activities** (did we complete the work plan?), and **outputs** (did we deliver the report?).

But clients care most about **outcomes** and **impact**, which are hardest to measure, take longest to materialise, and are most subject to confounding factors.

### Towards meaningful evaluation

Despite these challenges, evaluation matters. Some practical approaches:

1. **Agree criteria upfront**: Build success measures into the contract (links to Week 2)
2. **Track leading indicators**: Don't wait for final impact. Monitor early signs of change
3. **Acknowledge multiple perspectives**: Gather data from different stakeholders rather than assuming one "truth"
4. **Separate learning from proving**: Create safe spaces to discuss what didn't work
5. **Be realistic about attribution**: Use contribution analysis rather than claiming causation

:::reflect
Think about a project you've been involved with (consulting or otherwise). How was success evaluated? Who defined the criteria? What perspectives were included or excluded?
:::
